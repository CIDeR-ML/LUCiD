{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd101f4-b519-45da-a404-60b13f9c20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get project paths correctly\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent  # diffCherenkov root (one level up from notebooks)\n",
    "photonsim_root = project_root.parent / 'PhotonSim'  # PhotonSim root\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Add project paths\n",
    "sys.path.insert(0, str(project_root))\n",
    "sys.path.insert(0, str(project_root / 'tools'))\n",
    "\n",
    "print(f\"Current dir: {current_dir}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PhotonSim root: {photonsim_root}\")\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"Project root exists: {project_root.exists()}\")\n",
    "print(f\"Siren dir exists: {(project_root / 'siren').exists()}\")\n",
    "print(f\"Training dir exists: {(project_root / 'siren' / 'training').exists()}\")\n",
    "print(f\"PhotonSim root exists: {photonsim_root.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5ea93-0c2e-4b6f-9426-23024b12e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the refactored training modules with fallback strategies\n",
    "print(\"üì¶ Importing training modules...\")\n",
    "\n",
    "imported_successfully = False\n",
    "        \n",
    "try:\n",
    "    training_path = project_root / 'siren' / 'training'\n",
    "    if str(training_path) not in sys.path:\n",
    "        sys.path.insert(0, str(training_path))\n",
    "    \n",
    "    from trainer import SIRENTrainer, TrainingConfig\n",
    "    from dataset import PhotonSimDataset\n",
    "    from monitor import TrainingMonitor, LiveTrainingCallback\n",
    "    from analyzer import TrainingAnalyzer\n",
    "    \n",
    "    print(\"‚úÖ Manual imports from individual files successful\")\n",
    "    imported_successfully = True\n",
    "    \n",
    "except ImportError as e3:\n",
    "    print(f\"‚ùå Manual import failed: {e3}\")\n",
    "    print(\"\\nüö® All import strategies failed!\")\n",
    "    print(\"Please check:\")\n",
    "    print(f\"  1. Current working directory: {Path.cwd()}\")\n",
    "    print(f\"  2. Project root: {project_root}\")\n",
    "    print(f\"  3. Siren directory exists: {(project_root / 'siren').exists()}\")\n",
    "    print(f\"  4. Training directory exists: {(project_root / 'siren' / 'training').exists()}\")\n",
    "    raise ImportError(\"Could not import training modules with any strategy\")\n",
    "\n",
    "if imported_successfully:\n",
    "    print(\"‚úÖ All training modules imported successfully!\")\n",
    "    print(\"üöÄ Ready to start training workflow\")\n",
    "else:\n",
    "    raise ImportError(\"Failed to import training modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648964e-02b9-4711-bff6-48f3d9bf42da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the PhotonSim HDF5 lookup table\n",
    "h5_path = photonsim_root / 'output' / 'photon_lookup_table.h5'\n",
    "\n",
    "# Check if file exists\n",
    "if not h5_path.exists():\n",
    "    print(f\"‚ùå HDF5 file not found at {h5_path}\")\n",
    "    print(\"Please run the PhotonSim table generation first:\")\n",
    "    print(\"  cd ../PhotonSim\")\n",
    "    print(\"  python tools/table_generation/create_density_3d_table.py --data-dir data/mu-\")\n",
    "else:\n",
    "    print(f\"‚úì Found PhotonSim HDF5 file: {h5_path}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = PhotonSimDataset(h5_path, val_split=0.1)\n",
    "    \n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"  Data type: {dataset.data_type}\")\n",
    "    print(f\"  Total samples: {len(dataset.data['inputs']):,}\")\n",
    "    print(f\"  Train samples: {len(dataset.train_indices):,}\")\n",
    "    print(f\"  Val samples: {len(dataset.val_indices):,}\")\n",
    "    print(f\"  Energy range: {dataset.energy_range[0]:.0f}-{dataset.energy_range[1]:.0f} MeV\")\n",
    "    print(f\"  Angle range: {np.degrees(dataset.angle_range[0]):.1f}-{np.degrees(dataset.angle_range[1]):.1f} degrees\")\n",
    "    print(f\"  Distance range: {dataset.distance_range[0]:.0f}-{dataset.distance_range[1]:.0f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788d05c-99cd-4848-b0f7-de226976a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and configure dataset with default consistent normalization\n",
    "dataset = PhotonSimDataset(h5_path, val_split=0.1)\n",
    "\n",
    "print(\"‚úÖ Dataset configured with built-in consistent normalization\")\n",
    "print(f\"  ‚Ä¢ Input normalization: [-1, 1]\")\n",
    "print(f\"  ‚Ä¢ Target normalization: [0, 1] from log scale\")\n",
    "print(f\"  ‚Ä¢ Training and evaluation use identical normalization by default\")\n",
    "\n",
    "# Verify dataset is working correctly\n",
    "rng = jax.random.PRNGKey(42)\n",
    "sample_inputs, sample_targets = dataset.get_batch(100, rng, 'train', normalized=True)\n",
    "\n",
    "print(f\"\\nüß™ Dataset verification:\")\n",
    "print(f\"  Sample inputs shape: {sample_inputs.shape}\")\n",
    "print(f\"  Sample targets shape: {sample_targets.shape}\")\n",
    "print(f\"  Input range: [{sample_inputs.min():.3f}, {sample_inputs.max():.3f}]\")\n",
    "print(f\"  Target range: [{sample_targets.min():.3f}, {sample_targets.max():.3f}]\")\n",
    "print(f\"  ‚úÖ Normalization working correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b59ab-e13e-4ba0-ade0-a7a09758ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training mode configuration\n",
    "START_FRESH = False\n",
    "\n",
    "# Set up output directory\n",
    "output_dir = Path('output') / 'photonsim_siren_training'\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Directory exists: {output_dir.exists()}\")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "existing_checkpoints = list(output_dir.glob('*.npz'))\n",
    "existing_history = output_dir / 'training_history.json'\n",
    "\n",
    "if existing_checkpoints or existing_history.exists():\n",
    "    print(f\"\\nüîç Found existing training data:\")\n",
    "    if existing_history.exists():\n",
    "        import json\n",
    "        with open(existing_history, 'r') as f:\n",
    "            history = json.load(f)\n",
    "            if history.get('step'):\n",
    "                last_step = max(history['step'])\n",
    "                print(f\"  - Training history up to step {last_step}\")\n",
    "    \n",
    "    for checkpoint in existing_checkpoints:\n",
    "        print(f\"  - Checkpoint: {checkpoint.name}\")\n",
    "    \n",
    "    if START_FRESH:\n",
    "        print(f\"\\nüîÑ START_FRESH=True: Will clear existing data and start from scratch\")\n",
    "    else:\n",
    "        print(f\"\\n‚ñ∂Ô∏è  START_FRESH=False: Will resume from latest checkpoint\")\n",
    "else:\n",
    "    print(f\"\\n‚ú® No existing training data found. Starting fresh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533fb21-4aa2-46e2-a195-2200d78675e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration - now with built-in CProfSiren-style loss scaling\n",
    "config = TrainingConfig(\n",
    "    # Model architecture - same as CProfSiren\n",
    "    hidden_features=256,\n",
    "    hidden_layers=3,        # CProfSiren used 3 layers\n",
    "    w0=30.0,               # Standard SIREN frequency\n",
    "    \n",
    "    # Training parameters - adapted from CProfSiren\n",
    "    learning_rate=1e-4,     # Same as CProfSiren\n",
    "    weight_decay=0.0,       # CProfSiren didn't use weight decay\n",
    "    batch_size=10_000,      # Large batches for stable gradients\n",
    "    num_steps=20000,        # Total training steps\n",
    "    \n",
    "    # PATIENCE-BASED LR SCHEDULER\n",
    "    use_patience_scheduler=True,   # Enable patience-based LR\n",
    "    patience=20,                   # Reduce LR after 20 validations with no improvement\n",
    "    lr_reduction_factor=0.5,       # Cut LR in half when triggered\n",
    "    min_lr=1e-7,                   # Don't go below this\n",
    "    \n",
    "    # Optimizer settings\n",
    "    optimizer='adam',       # Same as CProfSiren\n",
    "    grad_clip_norm=0.0,     # No gradient clipping\n",
    "    \n",
    "    # Logging frequency\n",
    "    log_every=10,           # Log training progress\n",
    "    val_every=50,           # Check validation for patience\n",
    "    checkpoint_every=500,   # Save checkpoints\n",
    "    \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"üìä Training Configuration:\")\n",
    "print(f\"  ‚Ä¢ Architecture: {config.hidden_layers} layers √ó {config.hidden_features} features\")\n",
    "print(f\"  ‚Ä¢ Learning Rate: {config.learning_rate:.2e} (with patience-based scheduling)\")\n",
    "print(f\"  ‚Ä¢ Batch Size: {config.batch_size:,}\")\n",
    "print(f\"  ‚Ä¢ Total Steps: {config.num_steps:,}\")\n",
    "print(f\"  ‚Ä¢ Built-in CProfSiren-style loss scaling (√ó1000)\")\n",
    "print(f\"  ‚Ä¢ Consistent log-normalized targets by default\")\n",
    "print(f\"  ‚úÖ Ready for training with all improvements built-in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22abf2b4-5bfd-41dc-bf7f-95c6cb2cc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with improved defaults (no custom functions needed!)\n",
    "trainer = SIRENTrainer(\n",
    "    dataset=dataset,        # Uses consistent normalization by default\n",
    "    config=config,\n",
    "    output_dir=output_dir,\n",
    "    resume_from_checkpoint=not START_FRESH\n",
    ")\n",
    "\n",
    "# Clear checkpoints if starting fresh\n",
    "if START_FRESH:\n",
    "    print(\"üßπ Clearing existing checkpoints...\")\n",
    "    trainer.clear_checkpoints()\n",
    "    print(\"‚úÖ Starting with clean slate\")\n",
    "\n",
    "print(f\"‚úì Trainer initialized with improved defaults\")\n",
    "print(f\"‚úì Output directory: {output_dir}\")\n",
    "print(f\"‚úì JAX device: {trainer.device}\")\n",
    "\n",
    "# Check if we're resuming\n",
    "if trainer.start_step > 0:\n",
    "    print(f\"‚úì Resuming from step {trainer.start_step}\")\n",
    "    print(f\"‚úì Training history loaded with {len(trainer.history['train_loss'])} entries\")\n",
    "else:\n",
    "    print(f\"‚úì Starting fresh training from step 0\")\n",
    "\n",
    "print(f\"\\nüéØ Built-in improvements:\")\n",
    "print(f\"  ‚Ä¢ CProfSiren-style MSE loss with √ó1000 scaling\")\n",
    "print(f\"  ‚Ä¢ Proper SIREN output handling (first element of tuple)\")\n",
    "print(f\"  ‚Ä¢ Consistent log-normalized targets throughout\")\n",
    "print(f\"  ‚Ä¢ No custom training functions needed - it's all built-in!\")\n",
    "print(f\"  ‚úÖ Ready to train with trainer.train()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edddacc-898d-41fa-b239-5ea6eb5669b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ SIMPLE VERIFICATION: Check normalization consistency\n",
    "print(\"üîç Quick normalization check...\")\n",
    "\n",
    "# Test batch consistency\n",
    "rng = jax.random.PRNGKey(42)\n",
    "base_inputs, base_targets = dataset.get_batch(100, rng, 'train', normalized=True)\n",
    "consistent_inputs, consistent_targets = dataset.get_batch(100, rng, 'train', normalized=True)\n",
    "\n",
    "print(f\"Base dataset targets: {base_targets.min():.6f} to {base_targets.max():.6f}\")\n",
    "print(f\"Consistent dataset targets: {consistent_targets.min():.6f} to {consistent_targets.max():.6f}\")\n",
    "\n",
    "# Check if they match\n",
    "if jnp.allclose(base_targets, consistent_targets):\n",
    "    print(\"‚úÖ SUCCESS! Normalization is now consistent\")\n",
    "    print(\"   ‚Üí SIREN training and evaluation use identical [0,1] scales\")\n",
    "    print(\"   ‚Üí Plots should now match visually!\")\n",
    "else:\n",
    "    print(\"‚ùå Still inconsistent\")\n",
    "    \n",
    "print(\"\\nüöÄ Ready to proceed with training and analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543505a-aaca-4972-a252-b4f2f9c68199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up monitoring with live plotting\n",
    "monitor = TrainingMonitor(output_dir, live_plotting=True)\n",
    "\n",
    "# Create live callback for real-time plot updates during training\n",
    "live_callback = LiveTrainingCallback(\n",
    "    monitor, \n",
    "    update_every=50,   # Update data every 50 steps\n",
    "    plot_every=200     # Update plots every 200 steps\n",
    ")\n",
    "\n",
    "# Add callback to trainer for live monitoring\n",
    "trainer.add_callback(live_callback)\n",
    "\n",
    "print(\"‚úì Monitoring setup complete\")\n",
    "print(\"‚úì Live plotting enabled - plots will update during training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c397e88-e7da-4695-bdeb-c2e6d5071d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting SIREN training...\")\n",
    "history = trainer.train()\n",
    "\n",
    "print(\"\\n‚úì Training completed!\")\n",
    "print(f\"Final train loss: {history['train_loss'][-1]:.6f}\")\n",
    "if history['val_loss']:\n",
    "    print(f\"Final val loss: {history['val_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2dc988-7b94-4d7e-a92a-add315e6555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig = trainer.plot_training_history(save_path=output_dir / 'final_training_progress.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cosuvkjpz7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model (now with robust saving built-in)\n",
    "model_save_dir = output_dir / 'trained_model'\n",
    "weights_path, metadata_path = trainer.save_trained_model(model_save_dir, 'photonsim_siren')\n",
    "\n",
    "print(f\"‚úÖ Model saved successfully!\")\n",
    "print(f\"  Weights: {weights_path}\")\n",
    "print(f\"  Metadata: {metadata_path}\")\n",
    "\n",
    "# Display the saved metadata\n",
    "import json\n",
    "with open(metadata_path, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"\\nüìã Model Metadata:\")\n",
    "print(f\"  Energy range: {metadata['dataset_info']['energy_range']} MeV\")\n",
    "print(f\"  Angle range: {np.degrees(metadata['dataset_info']['angle_range'])} degrees\") \n",
    "print(f\"  Distance range: {metadata['dataset_info']['distance_range']} mm\")\n",
    "print(f\"  Model architecture: {metadata['model_config']['hidden_layers']} layers √ó {metadata['model_config']['hidden_features']} features\")\n",
    "print(f\"  Final training loss: {metadata['training_info']['final_train_loss']:.6f}\")\n",
    "if metadata['training_info']['final_val_loss']:\n",
    "    print(f\"  Final validation loss: {metadata['training_info']['final_val_loss']:.6f}\")\n",
    "print(f\"  ‚úÖ Model saved with all metadata and robust parameter handling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e0442e-55b3-49ca-8f74-1a4bdc8d51f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
