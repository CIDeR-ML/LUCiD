{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ **Training Workflow Complete!**\n",
    "\n",
    "This notebook demonstrates the complete SIREN training and evaluation workflow:\n",
    "\n",
    "1. **Data Loading**: PhotonSim HDF5 lookup table with proper normalization\n",
    "2. **Model Training**: JAX/Flax SIREN with linear-scale training (matching CProfSiren)\n",
    "3. **Model Saving**: Complete metadata preservation for inference\n",
    "4. **Model Loading**: Standalone inference capabilities with proper denormalization\n",
    "5. **Validation**: Visual comparison between SIREN predictions and original lookup table\n",
    "\n",
    "**Key Features:**\n",
    "- üîÑ **Normalization Handling**: Automatic input/output scaling for inference\n",
    "- üìä **Metadata Preservation**: Complete dataset info, training config, and normalization parameters\n",
    "- üöÄ **Easy Inference**: Simple prediction interface for single points or batches\n",
    "- üìà **Visualization**: Direct comparison with original lookup table data\n",
    "- üéØ **Accuracy**: High-fidelity reproduction of Cherenkov light patterns\n",
    "\n",
    "The trained model can now be used as a drop-in replacement for lookup table interpolation in diffCherenkov simulations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Key Improvements Based on CProfSiren Analysis\n",
    "\n",
    "This notebook has been updated to match the successful training approach from CProfSiren:\n",
    "\n",
    "1. **Linear Training**: Train on actual photon density values (not log-transformed)\n",
    "2. **Loss Scaling**: Multiply MSE loss by 1000 for better gradient flow\n",
    "3. **Output Squaring**: SIREN model squares output to ensure positive densities\n",
    "4. **Large Batches**: Use batch_size=65536 for stable training\n",
    "5. **Fixed LR Schedule**: StepLR with 10√ó drop at step 2000 (no patience-based updates)\n",
    "6. **Proper SIREN Handling**: Fixed tuple output issue `(output, coords)`\n",
    "\n",
    "These changes address the performance degradation and should restore good training behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir: /sdf/home/c/cjesus/Dev/diffCherenkov/notebooks\n",
      "Project root: /sdf/home/c/cjesus/Dev/diffCherenkov\n",
      "PhotonSim root: /sdf/home/c/cjesus/Dev/PhotonSim\n",
      "Project root exists: True\n",
      "Siren dir exists: True\n",
      "Training dir exists: True\n",
      "PhotonSim root exists: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get project paths correctly\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent  # diffCherenkov root (one level up from notebooks)\n",
    "photonsim_root = project_root.parent / 'PhotonSim'  # PhotonSim root\n",
    "\n",
    "# Add project paths\n",
    "sys.path.insert(0, str(project_root))\n",
    "sys.path.insert(0, str(project_root / 'tools'))\n",
    "\n",
    "print(f\"Current dir: {current_dir}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PhotonSim root: {photonsim_root}\")\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"Project root exists: {project_root.exists()}\")\n",
    "print(f\"Siren dir exists: {(project_root / 'siren').exists()}\")\n",
    "print(f\"Training dir exists: {(project_root / 'siren' / 'training').exists()}\")\n",
    "print(f\"PhotonSim root exists: {photonsim_root.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Importing training modules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "INFO:numexpr.utils:Note: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imported from training module directly\n",
      "‚úÖ All training modules imported successfully!\n",
      "üöÄ Ready to start training workflow\n"
     ]
    }
   ],
   "source": [
    "# Import the refactored training modules\n",
    "print(\"üì¶ Importing training modules...\")\n",
    "\n",
    "try:\n",
    "    # Add siren directory to path and import training module\n",
    "    siren_path = project_root / 'siren'\n",
    "    if str(siren_path) not in sys.path:\n",
    "        sys.path.insert(0, str(siren_path))\n",
    "    \n",
    "    from training import (\n",
    "        SIRENTrainer, \n",
    "        TrainingConfig, \n",
    "        PhotonSimDataset,\n",
    "        TrainingMonitor,\n",
    "        TrainingAnalyzer,\n",
    "        LiveTrainingCallback\n",
    "    )\n",
    "    print(\"‚úÖ Imported from training module directly\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    print(\"üîß Trying manual imports from individual files...\")\n",
    "    \n",
    "    # Import from individual module files\n",
    "    training_path = project_root / 'siren' / 'training'\n",
    "    if str(training_path) not in sys.path:\n",
    "        sys.path.insert(0, str(training_path))\n",
    "    \n",
    "    from trainer import SIRENTrainer, TrainingConfig\n",
    "    from dataset import PhotonSimDataset\n",
    "    from monitor import TrainingMonitor, LiveTrainingCallback\n",
    "    from analyzer import TrainingAnalyzer\n",
    "    \n",
    "    print(\"‚úÖ Manual imports from individual files successful\")\n",
    "\n",
    "print(\"‚úÖ All training modules imported successfully!\")\n",
    "print(\"üöÄ Ready to start training workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the data distribution to understand training challenges\n",
    "if 'dataset' in locals():\n",
    "    print(\"\\nüìä Data Analysis:\")\n",
    "    \n",
    "    # Check target value distribution\n",
    "    targets = dataset.data['targets']\n",
    "    targets_nonzero = targets[targets > 0]\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Total samples: {len(targets):,}\")\n",
    "    print(f\"  ‚Ä¢ Non-zero samples: {len(targets_nonzero):,} ({len(targets_nonzero)/len(targets)*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Target range: [{targets.min():.2e}, {targets.max():.2e}]\")\n",
    "    print(f\"  ‚Ä¢ Target mean: {targets.mean():.2e}\")\n",
    "    print(f\"  ‚Ä¢ Target std: {targets.std():.2e}\")\n",
    "    \n",
    "    # Check log-transformed targets\n",
    "    targets_log = dataset.data['targets_log']\n",
    "    print(f\"\\n  ‚Ä¢ Log target range: [{targets_log.min():.2f}, {targets_log.max():.2f}]\")\n",
    "    print(f\"  ‚Ä¢ Log target mean: {targets_log.mean():.2f}\")\n",
    "    \n",
    "    # Show distribution\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Linear scale histogram\n",
    "    ax1.hist(targets[targets > 0], bins=100, alpha=0.7)\n",
    "    ax1.set_xlabel('Photon Density')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_title('Target Distribution (Linear Scale)')\n",
    "    \n",
    "    # Log scale histogram  \n",
    "    ax2.hist(targets_log, bins=100, alpha=0.7, color='orange')\n",
    "    ax2.set_xlabel('Log10(Photon Density)')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_title('Target Distribution (Log Scale)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Insights:\")\n",
    "    print(\"  ‚Ä¢ Data has many small/zero values - importance sampling could help\")\n",
    "    print(\"  ‚Ä¢ Wide dynamic range suggests log-space training might be beneficial\")\n",
    "    print(\"  ‚Ä¢ But CProfSiren trained directly on linear values with MSE loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to the PhotonSim HDF5 lookup table\n",
    "h5_path = photonsim_root / 'output' / 'photon_lookup_table.h5'\n",
    "\n",
    "# Check if file exists and load dataset\n",
    "if not h5_path.exists():\n",
    "    print(f\"‚ùå HDF5 file not found at {h5_path}\")\n",
    "    print(\"Please run the PhotonSim table generation first:\")\n",
    "    print(\"  cd ../PhotonSim\")\n",
    "    print(\"  python tools/table_generation/create_density_3d_table.py --data-dir data/mu-\")\n",
    "else:\n",
    "    print(f\"‚úì Found PhotonSim HDF5 file: {h5_path}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = PhotonSimDataset(h5_path, val_split=0.1)\n",
    "    \n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"  Data type: {dataset.data_type}\")\n",
    "    print(f\"  Total samples: {len(dataset.data['inputs']):,}\")\n",
    "    print(f\"  Train samples: {len(dataset.train_indices):,}\")\n",
    "    print(f\"  Val samples: {len(dataset.val_indices):,}\")\n",
    "    print(f\"  Energy range: {dataset.energy_range[0]:.0f}-{dataset.energy_range[1]:.0f} MeV\")\n",
    "    print(f\"  Angle range: {np.degrees(dataset.angle_range[0]):.1f}-{np.degrees(dataset.angle_range[1]):.1f} degrees\")\n",
    "    print(f\"  Distance range: {dataset.distance_range[0]:.0f}-{dataset.distance_range[1]:.0f} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Training Parameters\n",
    "\n",
    "Set up the training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL FIX: Modify dataset to train on linear values (not log)\n",
    "# This matches the successful CProfSiren approach\n",
    "\n",
    "class LinearPhotonSimDataset:\n",
    "    \"\"\"Wrapper to make dataset return linear values instead of log values\"\"\"\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        # Copy necessary attributes directly\n",
    "        self.data = base_dataset.data\n",
    "        self.train_indices = base_dataset.train_indices\n",
    "        self.val_indices = base_dataset.val_indices\n",
    "        self.normalized_bounds = base_dataset.normalized_bounds\n",
    "        self.metadata = base_dataset.metadata\n",
    "        self.energy_range = base_dataset.energy_range\n",
    "        self.angle_range = base_dataset.angle_range\n",
    "        self.distance_range = base_dataset.distance_range\n",
    "    \n",
    "    def get_batch(self, batch_size, rng, split='train', normalized=True):\n",
    "        # Get normalized inputs but LINEAR targets\n",
    "        if split == 'train':\n",
    "            indices = self.train_indices\n",
    "        else:\n",
    "            indices = self.val_indices\n",
    "            \n",
    "        # Random sampling\n",
    "        batch_indices = jax.random.choice(rng, indices, shape=(batch_size,))\n",
    "        \n",
    "        # Get normalized inputs\n",
    "        inputs = self.data['inputs_normalized'][batch_indices]\n",
    "        # Get LINEAR targets (not log!)\n",
    "        targets = self.data['targets'][batch_indices]\n",
    "        \n",
    "        return jnp.array(inputs), jnp.array(targets)\n",
    "    \n",
    "    def get_sample_input(self):\n",
    "        return jnp.array(self.data['inputs_normalized'][:1])\n",
    "    \n",
    "    @property\n",
    "    def has_validation(self):\n",
    "        return len(self.val_indices) > 0\n",
    "\n",
    "# Path to the PhotonSim HDF5 lookup table\n",
    "h5_path = photonsim_root / 'output' / 'photon_lookup_table.h5'\n",
    "\n",
    "# Check if file exists\n",
    "if not h5_path.exists():\n",
    "    print(f\"‚ùå HDF5 file not found at {h5_path}\")\n",
    "    print(\"Please run the PhotonSim table generation first:\")\n",
    "    print(\"  cd ../PhotonSim\")\n",
    "    print(\"  python tools/table_generation/create_density_3d_table.py --data-dir data/mu-\")\n",
    "else:\n",
    "    print(f\"‚úì Found PhotonSim HDF5 file: {h5_path}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = PhotonSimDataset(h5_path, val_split=0.1)\n",
    "    \n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"  Data type: {dataset.data_type}\")\n",
    "    print(f\"  Total samples: {len(dataset.data['inputs']):,}\")\n",
    "    print(f\"  Train samples: {len(dataset.train_indices):,}\")\n",
    "    print(f\"  Val samples: {len(dataset.val_indices):,}\")\n",
    "    print(f\"  Energy range: {dataset.energy_range[0]:.0f}-{dataset.energy_range[1]:.0f} MeV\")\n",
    "    print(f\"  Angle range: {np.degrees(dataset.angle_range[0]):.1f}-{np.degrees(dataset.angle_range[1]):.1f} degrees\")\n",
    "    print(f\"  Distance range: {dataset.distance_range[0]:.0f}-{dataset.distance_range[1]:.0f} mm\")\n",
    "\n",
    "# Wrap the dataset to use linear values\n",
    "linear_dataset = LinearPhotonSimDataset(dataset)\n",
    "\n",
    "print(\"‚úÖ Dataset configured for linear training (matching CProfSiren)\")\n",
    "print(f\"  ‚Ä¢ Input normalization: [-1, 1]\")\n",
    "print(f\"  ‚Ä¢ Target values: Linear scale (not log)\")\n",
    "print(f\"  ‚Ä¢ Ready for training with MSE loss √ó 1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training configuration with PATIENCE-BASED learning rate scheduling\n",
    "config = TrainingConfig(\n",
    "    # Model architecture - same as CProfSiren\n",
    "    hidden_features=256,\n",
    "    hidden_layers=3,        # CProfSiren used 3 layers\n",
    "    w0=30.0,               # Standard SIREN frequency\n",
    "    \n",
    "    # Training parameters - adapted from CProfSiren\n",
    "    learning_rate=1e-4,     # Same as CProfSiren\n",
    "    weight_decay=0.0,       # CProfSiren didn't use weight decay\n",
    "    batch_size=10_000,#65536,       # Large batches (as large as memory allows)\n",
    "    num_steps=25000,        # More steps to see patience in action\n",
    "    \n",
    "    # PATIENCE-BASED LR SCHEDULER - much better than fixed!\n",
    "    use_patience_scheduler=True,   # Enable patience-based LR\n",
    "    patience=20,                   # Reduce LR after 20 validations with no improvement\n",
    "    lr_reduction_factor=0.5,       # Cut LR in half when triggered\n",
    "    min_lr=5e-6,                   # Don't go below this\n",
    "    \n",
    "    # Optimizer settings\n",
    "    optimizer='adam',       # Same as CProfSiren\n",
    "    grad_clip_norm=0.0,    # CProfSiren didn't use gradient clipping\n",
    "    \n",
    "    # Logging frequency\n",
    "    log_every=10,          # CProfSiren logged every 10 steps\n",
    "    val_every=50,          # Check validation more frequently for patience\n",
    "    checkpoint_every=500,  # Save periodically\n",
    "    \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"üìä Training Configuration (CProfSiren-inspired with Patience LR):\")\n",
    "print(f\"  ‚Ä¢ Architecture: {config.hidden_layers} layers √ó {config.hidden_features} features\")\n",
    "print(f\"  ‚Ä¢ Initial LR: {config.learning_rate:.2e}\")\n",
    "print(f\"  ‚Ä¢ Batch Size: {config.batch_size:,} (large for stable gradients)\")\n",
    "print(f\"  ‚Ä¢ Total Steps: {config.num_steps:,}\")\n",
    "print(f\"\\nüéØ Patience-based LR Schedule:\")\n",
    "print(f\"  ‚Ä¢ Patience: {config.patience} validation checks\")\n",
    "print(f\"  ‚Ä¢ LR reduction: √ó{config.lr_reduction_factor} when triggered\")\n",
    "print(f\"  ‚Ä¢ Minimum LR: {config.min_lr:.2e}\")\n",
    "print(f\"  ‚Ä¢ Validation every: {config.val_every} steps\")\n",
    "print(\"\\n‚ú® Advantages over fixed schedule:\")\n",
    "print(\"  ‚Üí Adapts to actual training progress\")\n",
    "print(\"  ‚Üí Won't reduce LR if still improving\")\n",
    "print(\"  ‚Üí More robust to different datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training mode configuration\n",
    "START_FRESH = True\n",
    "\n",
    "# Set up output directory\n",
    "output_dir = Path('output') / 'photonsim_siren_training'\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Directory exists: {output_dir.exists()}\")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "existing_checkpoints = list(output_dir.glob('*.npz'))\n",
    "existing_history = output_dir / 'training_history.json'\n",
    "\n",
    "if existing_checkpoints or existing_history.exists():\n",
    "    print(f\"\\nüîç Found existing training data:\")\n",
    "    if existing_history.exists():\n",
    "        import json\n",
    "        with open(existing_history, 'r') as f:\n",
    "            history = json.load(f)\n",
    "            if history.get('step'):\n",
    "                last_step = max(history['step'])\n",
    "                print(f\"  - Training history up to step {last_step}\")\n",
    "    \n",
    "    for checkpoint in existing_checkpoints:\n",
    "        print(f\"  - Checkpoint: {checkpoint.name}\")\n",
    "    \n",
    "    if START_FRESH:\n",
    "        print(f\"\\nüîÑ START_FRESH=True: Will clear existing data and start from scratch\")\n",
    "    else:\n",
    "        print(f\"\\n‚ñ∂Ô∏è  START_FRESH=False: Will resume from latest checkpoint\")\n",
    "else:\n",
    "    print(f\"\\n‚ú® No existing training data found. Starting fresh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom training with PROPER patience-based LR scheduling\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from typing import NamedTuple, Any\n",
    "\n",
    "class TrainingState(NamedTuple):\n",
    "    \"\"\"Extended training state with patience tracking\"\"\"\n",
    "    params: Any\n",
    "    opt_state: Any\n",
    "    best_loss: float\n",
    "    patience_count: int\n",
    "    lr_index: int\n",
    "\n",
    "# Learning rate schedule with patience\n",
    "def create_patience_lr_schedule(base_lr, factor, patience, min_lr):\n",
    "    \"\"\"Create a patience-based learning rate schedule\"\"\"\n",
    "    lr_values = [base_lr]\n",
    "    current_lr = base_lr\n",
    "    while current_lr > min_lr:\n",
    "        current_lr *= factor\n",
    "        lr_values.append(max(current_lr, min_lr))\n",
    "    \n",
    "    # Return a function that selects LR based on index\n",
    "    def schedule(step, lr_index):\n",
    "        return lr_values[min(lr_index, len(lr_values) - 1)]\n",
    "    \n",
    "    return schedule, lr_values\n",
    "\n",
    "# Create the schedule\n",
    "lr_schedule_fn, lr_values = create_patience_lr_schedule(\n",
    "    config.learning_rate, \n",
    "    config.lr_reduction_factor,\n",
    "    config.patience,\n",
    "    config.min_lr\n",
    ")\n",
    "\n",
    "print(f\"üìà Learning rate schedule: {[f'{lr:.2e}' for lr in lr_values[:5]]}...\")\n",
    "\n",
    "# Custom training functions\n",
    "@jax.jit\n",
    "def train_step_with_patience(state, batch, lr):\n",
    "    \"\"\"Training step with explicit learning rate\"\"\"\n",
    "    inputs, targets = batch\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        # SIREN returns tuple (output, coords) - take first element\n",
    "        output, _ = trainer.model.apply({'params': params}, inputs)\n",
    "        \n",
    "        # Ensure proper shape\n",
    "        if output.ndim == 1:\n",
    "            output = output[:, None]\n",
    "            \n",
    "        # MSE loss with scaling\n",
    "        loss = jnp.mean((output - targets) ** 2) * 1000.0\n",
    "        return loss\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "    \n",
    "    # Create optimizer with current learning rate\n",
    "    optimizer = optax.adam(learning_rate=lr)\n",
    "    \n",
    "    # Update parameters\n",
    "    updates, new_opt_state = optimizer.update(grads, state.opt_state, state.params)\n",
    "    new_params = optax.apply_updates(state.params, updates)\n",
    "    \n",
    "    new_state = TrainingState(\n",
    "        params=new_params,\n",
    "        opt_state=new_opt_state,\n",
    "        best_loss=state.best_loss,\n",
    "        patience_count=state.patience_count,\n",
    "        lr_index=state.lr_index\n",
    "    )\n",
    "    \n",
    "    return new_state, loss\n",
    "\n",
    "@jax.jit\n",
    "def eval_step_with_patience(params, batch, model_apply):\n",
    "    \"\"\"Evaluation step\"\"\"\n",
    "    inputs, targets = batch\n",
    "    \n",
    "    # SIREN returns tuple (output, coords) - take first element\n",
    "    output, _ = model_apply({'params': params}, inputs)\n",
    "    \n",
    "    # Ensure proper shape\n",
    "    if output.ndim == 1:\n",
    "        output = output[:, None]\n",
    "        \n",
    "    # MSE loss with scaling\n",
    "    loss = jnp.mean((output - targets) ** 2) * 1000.0\n",
    "    return loss\n",
    "\n",
    "print(\"‚úÖ Custom training with patience-based LR scheduling ready!\")\n",
    "print(\"  ‚Ä¢ Preserves optimizer state correctly\")\n",
    "print(\"  ‚Ä¢ Reduces LR only when validation plateaus\")\n",
    "print(\"  ‚Ä¢ Implements CProfSiren-style loss scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with LINEAR dataset (not log)\n",
    "trainer = SIRENTrainer(\n",
    "    dataset=linear_dataset,  # Use linear dataset!\n",
    "    config=config,\n",
    "    output_dir=output_dir,\n",
    "    resume_from_checkpoint=not START_FRESH\n",
    ")\n",
    "\n",
    "# Override the training functions with our custom ones\n",
    "trainer._create_train_step = lambda: train_step_cprofstyle\n",
    "trainer._create_eval_step = lambda: eval_step_cprofstyle\n",
    "\n",
    "# Clear checkpoints if starting fresh\n",
    "if START_FRESH:\n",
    "    print(\"üßπ Clearing existing checkpoints...\")\n",
    "    trainer.clear_checkpoints()\n",
    "    print(\"‚úÖ Starting with clean slate\")\n",
    "\n",
    "print(f\"‚úì Trainer initialized with CProfSiren-style training\")\n",
    "print(f\"‚úì Output directory: {output_dir}\")\n",
    "print(f\"‚úì JAX device: {trainer.device}\")\n",
    "\n",
    "# Check if we're resuming\n",
    "if trainer.start_step > 0:\n",
    "    print(f\"‚úì Resuming from step {trainer.start_step}\")\n",
    "    print(f\"‚úì Training history loaded with {len(trainer.history['train_loss'])} entries\")\n",
    "else:\n",
    "    print(f\"‚úì Starting fresh training from step 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Trainer and Monitor\n",
    "\n",
    "Create the trainer and set up monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with resume option\n",
    "trainer = SIRENTrainer(\n",
    "    dataset=dataset,\n",
    "    config=config,\n",
    "    output_dir=output_dir,\n",
    "    resume_from_checkpoint=not START_FRESH  # Resume unless starting fresh\n",
    ")\n",
    "\n",
    "# Clear checkpoints if starting fresh\n",
    "if START_FRESH:\n",
    "    print(\"üßπ Clearing existing checkpoints...\")\n",
    "    trainer.clear_checkpoints()\n",
    "    print(\"‚úÖ Starting with clean slate\")\n",
    "\n",
    "print(f\"‚úì Trainer initialized\")\n",
    "print(f\"‚úì Output directory: {output_dir}\")\n",
    "print(f\"‚úì JAX device: {trainer.device}\")\n",
    "\n",
    "# Check if we're resuming\n",
    "if trainer.start_step > 0:\n",
    "    print(f\"‚úì Resuming from step {trainer.start_step}\")\n",
    "    print(f\"‚úì Training history loaded with {len(trainer.history['train_loss'])} entries\")\n",
    "else:\n",
    "    print(f\"‚úì Starting fresh training from step 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Notes\n",
    "\n",
    "With the CProfSiren-style configuration:\n",
    "- Loss values will be ~1000√ó larger due to scaling (this is expected)\n",
    "- Training on linear values captures the full dynamic range\n",
    "- The SIREN model squares its output internally to ensure positive densities\n",
    "- Large batch sizes provide stable gradients\n",
    "- StepLR will drop learning rate aggressively at step 2000\n",
    "\n",
    "Monitor for:\n",
    "- Steady loss decrease in the first 2000 steps\n",
    "- Sharp improvement after LR drop at step 2000\n",
    "- Validation loss tracking training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "\n",
    "Start training with live monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up monitoring with live plotting\n",
    "monitor = TrainingMonitor(output_dir, live_plotting=True)\n",
    "\n",
    "# Create live callback for real-time plot updates during training\n",
    "live_callback = LiveTrainingCallback(\n",
    "    monitor, \n",
    "    update_every=50,   # Update data every 50 steps\n",
    "    plot_every=200     # Update plots every 200 steps\n",
    ")\n",
    "\n",
    "# Add callback to trainer for live monitoring\n",
    "trainer.add_callback(live_callback)\n",
    "\n",
    "print(\"‚úì Monitoring setup complete\")\n",
    "print(\"‚úì Live plotting enabled - plots will update during training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting SIREN training...\")\n",
    "history = trainer.train()\n",
    "\n",
    "print(\"\\n‚úì Training completed!\")\n",
    "print(f\"Final train loss: {history['train_loss'][-1]:.6f}\")\n",
    "if history['val_loss']:\n",
    "    print(f\"Final val loss: {history['val_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig = trainer.plot_training_history(save_path=output_dir / 'final_training_progress.png')\n",
    "plt.show()\n",
    "\n",
    "# Also plot monitoring dashboard\n",
    "print(\"\\nüìä Training Progress Dashboard:\")\n",
    "monitor_fig = monitor.plot_progress(save_path=output_dir / 'training_dashboard.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig = trainer.plot_training_history(save_path=output_dir / 'final_training_progress.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Model Performance\n",
    "\n",
    "Use the analyzer to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create slice comparison plots\n",
    "fig_slices = analyzer.plot_lookup_table_slices(save_path=output_dir / 'lookup_table_slices.png', figsize=(16,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Model Predictions\n",
    "\n",
    "Test the model on specific energy/angle/distance combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Trained Model with Metadata\n",
    "\n",
    "Save the trained model with all necessary metadata for inference, including normalization parameters and dataset information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export analysis results\n",
    "# analyzer.export_results(output_dir / 'analysis_results.json')\n",
    "\n",
    "# # Export monitoring data\n",
    "# monitor.export_data(output_dir / 'monitoring_data.json')\n",
    "\n",
    "# print(\"‚úì Results exported to:\")\n",
    "# print(f\"  Model checkpoint: {output_dir / 'final_model.npz'}\")\n",
    "# print(f\"  Training config: {output_dir / 'config.json'}\")\n",
    "# print(f\"  Training history: {output_dir / 'training_history.json'}\")\n",
    "# print(f\"  Analysis results: {output_dir / 'analysis_results.json'}\")\n",
    "# print(f\"  Monitoring data: {output_dir / 'monitoring_data.json'}\")\n",
    "# print(f\"  Plots: {output_dir / '*.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create angular profile comparison plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Angular Profiles: SIREN vs Lookup Table', fontsize=14)\n",
    "\n",
    "energies_for_profiles = [300, 500, 800]\n",
    "fixed_distance = 2000  # mm\n",
    "\n",
    "for i, energy in enumerate(energies_for_profiles):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Find closest energy in lookup table\n",
    "    energy_idx = np.argmin(np.abs(energy_centers - energy))\n",
    "    actual_energy = energy_centers[energy_idx]\n",
    "    \n",
    "    if np.abs(actual_energy - energy) > 100:\n",
    "        ax.text(0.5, 0.5, f'No data near\\\\n{energy} MeV', \n",
    "               ha='center', va='center', transform=ax.transAxes)\n",
    "        continue\n",
    "    \n",
    "    # Find closest distance index\n",
    "    distance_idx = np.argmin(np.abs(distance_centers - fixed_distance))\n",
    "    actual_distance = distance_centers[distance_idx]\n",
    "    \n",
    "    # Get lookup table angular profile at this energy and distance\n",
    "    table_profile = density_table[energy_idx, :, distance_idx]\n",
    "    \n",
    "    # Create SIREN predictions for same points\n",
    "    coords_1d = np.array([[actual_energy, angle, actual_distance] for angle in angle_centers])\n",
    "    siren_profile = predictor.predict_batch(coords_1d)\n",
    "    \n",
    "    # Convert angles to degrees for plotting\n",
    "    angles_deg = np.degrees(angle_centers)\n",
    "    \n",
    "    # Plot both profiles\n",
    "    valid_mask = (table_profile > 1e-10) & (siren_profile > 1e-10)\n",
    "    \n",
    "    if np.sum(valid_mask) > 5:\n",
    "        ax.plot(angles_deg[valid_mask], table_profile[valid_mask], \n",
    "               'o-', alpha=0.8, label='Lookup Table', markersize=4, linewidth=2, color='blue')\n",
    "        ax.plot(angles_deg[valid_mask], siren_profile[valid_mask], \n",
    "               's--', alpha=0.8, label='SIREN Model', markersize=3, linewidth=2, color='red')\n",
    "        \n",
    "        # Mark Cherenkov angle (approximately 43 degrees for water)\n",
    "        ax.axvline(43, color='green', linestyle=':', alpha=0.7, label='Cherenkov angle')\n",
    "        \n",
    "        ax.set_xlabel('Angle (degrees)')\n",
    "        ax.set_ylabel('Photon Density')\n",
    "        ax.set_title(f'{actual_energy:.0f} MeV\\\\n(d = {actual_distance:.0f} mm)')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlim(angles_deg.min(), angles_deg.max())\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        if i == 0:  # Show legend only on first plot\n",
    "            ax.legend()\n",
    "        \n",
    "        # Calculate and display correlation\n",
    "        from scipy.stats import pearsonr\n",
    "        corr, _ = pearsonr(table_profile[valid_mask], siren_profile[valid_mask])\n",
    "        ax.text(0.05, 0.95, f'R = {corr:.3f}', transform=ax.transAxes,\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Insufficient\\\\nvalid data', ha='center', va='center', \n",
    "               transform=ax.transAxes)\n",
    "        ax.set_title(f'{energy} MeV')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'angular_profile_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Angular profile comparison saved to: {output_dir / 'angular_profile_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'photonsim_root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m energies_to_plot \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m600\u001b[39m, \u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m1000\u001b[39m]  \u001b[38;5;66;03m# MeV\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the original HDF5 file\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m h5_path \u001b[38;5;241m=\u001b[39m \u001b[43mphotonsim_root\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphoton_lookup_table.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(h5_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Load full density table and coordinates\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     density_table \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/photon_table_density\u001b[39m\u001b[38;5;124m'\u001b[39m][:]  \u001b[38;5;66;03m# Shape: (n_energy, n_angle, n_distance)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'photonsim_root' is not defined"
     ]
    }
   ],
   "source": [
    "# Load original lookup table for comparison\n",
    "import h5py\n",
    "\n",
    "# Define energies to compare\n",
    "energies_to_plot = [200, 400, 600, 800, 1000]  # MeV\n",
    "\n",
    "# Load the original HDF5 file\n",
    "h5_path = photonsim_root / 'output' / 'photon_lookup_table.h5'\n",
    "\n",
    "with h5py.File(h5_path, 'r') as f:\n",
    "    # Load full density table and coordinates\n",
    "    density_table = f['data/photon_table_density'][:]  # Shape: (n_energy, n_angle, n_distance)\n",
    "    energy_centers = f['coordinates/energy_centers'][:]\n",
    "    angle_centers = f['coordinates/angle_centers'][:]\n",
    "    distance_centers = f['coordinates/distance_centers'][:]\n",
    "\n",
    "print(f\"üìä Loaded lookup table with shape: {density_table.shape}\")\n",
    "print(f\"  Energy range: {energy_centers.min():.0f} to {energy_centers.max():.0f} MeV\")\n",
    "print(f\"  Angle range: {np.degrees(angle_centers.min()):.1f} to {np.degrees(angle_centers.max()):.1f} degrees\")\n",
    "print(f\"  Distance range: {distance_centers.min():.0f} to {distance_centers.max():.0f} mm\")\n",
    "\n",
    "# Create figure with 2 rows √ó 5 columns\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "fig.suptitle('SIREN vs Lookup Table Comparison: 2D Slices at Fixed Energies', fontsize=16)\n",
    "\n",
    "for i, energy in enumerate(energies_to_plot):\n",
    "    # Find closest energy index in lookup table\n",
    "    energy_idx = np.argmin(np.abs(energy_centers - energy))\n",
    "    actual_energy = energy_centers[energy_idx]\n",
    "    \n",
    "    if np.abs(actual_energy - energy) > 100:  # Skip if too far\n",
    "        axes[0, i].text(0.5, 0.5, f'No data near\\\\n{energy} MeV', \n",
    "                       ha='center', va='center', transform=axes[0, i].transAxes)\n",
    "        axes[1, i].text(0.5, 0.5, f'No data near\\\\n{energy} MeV', \n",
    "                       ha='center', va='center', transform=axes[1, i].transAxes)\n",
    "        continue\n",
    "    \n",
    "    # Get lookup table slice\n",
    "    table_slice = density_table[energy_idx, :, :]  # Shape: (n_angle, n_distance)\n",
    "    \n",
    "    # Create SIREN prediction for the same grid\n",
    "    angle_mesh, distance_mesh = np.meshgrid(angle_centers, distance_centers, indexing='ij')\n",
    "    energy_grid = np.full_like(angle_mesh, actual_energy)\n",
    "    \n",
    "    # Stack coordinates for SIREN prediction\n",
    "    eval_coords = np.stack([\n",
    "        energy_grid.flatten(),\n",
    "        angle_mesh.flatten(),\n",
    "        distance_mesh.flatten()\n",
    "    ], axis=-1)\n",
    "    \n",
    "    # Get SIREN predictions\n",
    "    siren_predictions = predictor.predict_batch(eval_coords)\n",
    "    siren_slice = siren_predictions.reshape(angle_mesh.shape)\n",
    "    \n",
    "    # Convert angles to degrees for plotting\n",
    "    angle_mesh_deg = np.degrees(angle_mesh)\n",
    "    \n",
    "    # Plot lookup table (top row)\n",
    "    ax_table = axes[0, i]\n",
    "    \n",
    "    # Use log scale for better visualization\n",
    "    table_slice_plot = np.where(table_slice > 1e-10, table_slice, np.nan)\n",
    "    im1 = ax_table.pcolormesh(angle_mesh_deg, distance_mesh, table_slice_plot, \n",
    "                             cmap='viridis', shading='auto',\n",
    "                             norm=plt.LogNorm(vmin=1e-2, vmax=table_slice.max()))\n",
    "    \n",
    "    ax_table.set_title(f'Lookup Table\\\\n{actual_energy:.0f} MeV')\n",
    "    ax_table.set_xlabel('Angle (degrees)')\n",
    "    ax_table.set_ylabel('Distance (mm)')\n",
    "    \n",
    "    if i == 4:  # Add colorbar to last plot\n",
    "        cbar1 = plt.colorbar(im1, ax=ax_table)\n",
    "        cbar1.set_label('Photon Density')\n",
    "    \n",
    "    # Plot SIREN predictions (bottom row)\n",
    "    ax_siren = axes[1, i]\n",
    "    \n",
    "    siren_slice_plot = np.where(siren_slice > 1e-10, siren_slice, np.nan)\n",
    "    im2 = ax_siren.pcolormesh(angle_mesh_deg, distance_mesh, siren_slice_plot, \n",
    "                             cmap='viridis', shading='auto',\n",
    "                             norm=plt.LogNorm(vmin=1e-2, vmax=siren_slice.max()))\n",
    "    \n",
    "    ax_siren.set_title(f'SIREN Model\\\\n{actual_energy:.0f} MeV')\n",
    "    ax_siren.set_xlabel('Angle (degrees)')\n",
    "    ax_siren.set_ylabel('Distance (mm)')\n",
    "    \n",
    "    if i == 4:  # Add colorbar to last plot\n",
    "        cbar2 = plt.colorbar(im2, ax=ax_siren)\n",
    "        cbar2.set_label('Photon Density')\n",
    "    \n",
    "    # Print comparison stats\n",
    "    print(f\"\\\\nüîç Energy {actual_energy:.0f} MeV:\")\n",
    "    print(f\"  Table range: {table_slice.min():.2e} to {table_slice.max():.2e}\")\n",
    "    print(f\"  SIREN range: {siren_slice.min():.2e} to {siren_slice.max():.2e}\")\n",
    "    print(f\"  SIREN/Table ratio: {siren_slice.mean()/table_slice.mean():.2f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'siren_vs_lookup_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\\\n‚úÖ Comparison plot saved to: {output_dir / 'siren_vs_lookup_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the inference module\n",
    "sys.path.append(str(project_root / 'siren' / 'training'))\n",
    "from inference import SIRENPredictor\n",
    "\n",
    "# Load the saved model\n",
    "model_base_path = model_save_dir / 'photonsim_siren'\n",
    "predictor = SIRENPredictor(model_base_path)\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# Test single prediction\n",
    "energy = 500  # MeV\n",
    "angle = np.radians(45)  # radians \n",
    "distance = 2000  # mm\n",
    "\n",
    "density = predictor.predict(energy, angle, distance)\n",
    "print(f\"\\nüîÆ Single Prediction Test:\")\n",
    "print(f\"  Input: E={energy} MeV, Œ∏={np.degrees(angle):.1f}¬∞, d={distance} mm\")\n",
    "print(f\"  Predicted density: {density:.2e} photons/mm¬≤\")\n",
    "\n",
    "# Test batch prediction\n",
    "test_inputs = np.array([\n",
    "    [400, np.radians(30), 1500],\n",
    "    [500, np.radians(45), 2000], \n",
    "    [600, np.radians(60), 2500]\n",
    "])\n",
    "\n",
    "batch_densities = predictor.predict_batch(test_inputs)\n",
    "print(f\"\\nüìä Batch Prediction Test:\")\n",
    "for i, (inp, dens) in enumerate(zip(test_inputs, batch_densities)):\n",
    "    print(f\"  Input {i+1}: E={inp[0]:.0f} MeV, Œ∏={np.degrees(inp[1]):.1f}¬∞, d={inp[2]:.0f} mm ‚Üí {dens:.2e}\")\n",
    "\n",
    "# Display model info\n",
    "info = predictor.get_info()\n",
    "print(f\"\\nüìã Loaded Model Info:\")\n",
    "print(f\"  Energy range: {info['dataset_info']['energy_range']} MeV\")\n",
    "print(f\"  Angle range: {np.degrees(info['dataset_info']['angle_range'])} degrees\")\n",
    "print(f\"  Distance range: {info['dataset_info']['distance_range']} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model with complete metadata\n",
    "model_save_dir = output_dir / 'trained_model'\n",
    "weights_path, metadata_path = trainer.save_trained_model(model_save_dir, 'photonsim_siren')\n",
    "\n",
    "print(f\"‚úÖ Model saved successfully!\")\n",
    "print(f\"  Weights: {weights_path}\")\n",
    "print(f\"  Metadata: {metadata_path}\")\n",
    "\n",
    "# Display the saved metadata\n",
    "import json\n",
    "with open(metadata_path, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"\\nüìã Model Metadata:\")\n",
    "print(f\"  Energy range: {metadata['dataset_info']['energy_range']} MeV\")\n",
    "print(f\"  Angle range: {np.degrees(metadata['dataset_info']['angle_range'])} degrees\") \n",
    "print(f\"  Distance range: {metadata['dataset_info']['distance_range']} mm\")\n",
    "print(f\"  Model architecture: {metadata['model_config']['hidden_layers']} layers √ó {metadata['model_config']['hidden_features']} features\")\n",
    "print(f\"  Final training loss: {metadata['training_info']['final_train_loss']:.6f}\")\n",
    "print(f\"  Final validation loss: {metadata['training_info']['final_val_loss']:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
