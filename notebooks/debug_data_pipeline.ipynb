{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Data Pipeline: Scale Consistency Check\n",
    "\n",
    "This notebook systematically checks the scales of:\n",
    "- A) Values in the original lookup table\n",
    "- B) Values sampled and used for training\n",
    "- C) SIREN predictions during training\n",
    "- D) Values used during plotting (analyzer)\n",
    "\n",
    "Goal: Identify where the scale mismatch occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from pathlib import Path\n",
    "\n",
    "# needed to avoid crash related to DNN library\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Add project paths (now we're in notebooks/ folder)\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'siren' / 'training'))\n",
    "\n",
    "from siren.training.dataset import PhotonSimDataset\n",
    "from siren.training.trainer import SIRENTrainer, TrainingConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Original Lookup Table Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== A) ORIGINAL LOOKUP TABLE VALUES ===\n",
      "Table shape: (91, 500, 500)\n",
      "Table min: 0.000000e+00\n",
      "Table max: 9.871154e+02\n",
      "Table mean: 1.354046e+00\n",
      "Table median: 0.000000e+00\n",
      "Non-zero values: 5,659,770/22,750,000\n",
      "\n",
      "Test point: E=550 MeV, θ=43.0°, d=5010 mm\n",
      "Original test value: 0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# Load original HDF5 file\n",
    "data_path = Path('/sdf/home/c/cjesus/Dev/PhotonSim/output/photon_lookup_table.h5')\n",
    "\n",
    "print(\"=== A) ORIGINAL LOOKUP TABLE VALUES ===\")\n",
    "with h5py.File(data_path, 'r') as f:\n",
    "    density_table = f['data/photon_table_density'][:]\n",
    "    energy_centers = f['coordinates/energy_centers'][:]\n",
    "    angle_centers = f['coordinates/angle_centers'][:]\n",
    "    distance_centers = f['coordinates/distance_centers'][:]\n",
    "\n",
    "print(f\"Table shape: {density_table.shape}\")\n",
    "print(f\"Table min: {density_table.min():.6e}\")\n",
    "print(f\"Table max: {density_table.max():.6e}\")\n",
    "print(f\"Table mean: {density_table.mean():.6e}\")\n",
    "print(f\"Table median: {np.median(density_table):.6e}\")\n",
    "print(f\"Non-zero values: {np.sum(density_table > 0):,}/{density_table.size:,}\")\n",
    "\n",
    "# Sample specific test point for tracking\n",
    "test_energy_idx = len(energy_centers) // 2  # 500 MeV\n",
    "test_angle_idx = np.argmin(np.abs(angle_centers - np.radians(43)))  # ~43 degrees (Cherenkov)\n",
    "test_dist_idx = len(distance_centers) // 2   # Middle distance\n",
    "\n",
    "test_coords = [energy_centers[test_energy_idx], angle_centers[test_angle_idx], distance_centers[test_dist_idx]]\n",
    "original_test_value = density_table[test_energy_idx, test_angle_idx, test_dist_idx]\n",
    "\n",
    "print(f\"\\nTest point: E={test_coords[0]:.0f} MeV, θ={np.degrees(test_coords[1]):.1f}°, d={test_coords[2]:.0f} mm\")\n",
    "print(f\"Original test value: {original_test_value:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Dataset Loading and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== B) DATASET AFTER LOADING AND FILTERING ===\n",
      "Filtered data shape: (5659770, 3)\n",
      "Raw targets min: 2.533042e-03\n",
      "Raw targets max: 9.871154e+02\n",
      "Raw targets mean: 5.442721e+00\n",
      "Raw targets median: 2.536646e-01\n",
      "\n",
      "Closest point in dataset: E=780 MeV, θ=43.0°, d=4970 mm\n",
      "Filtered test value: 7.425478e-03\n",
      "Value preservation ratio: inf (should be ~1.0)\n",
      "\n",
      "Log-normalized targets min: -2.596\n",
      "Log-normalized targets max: 2.994\n",
      "Log-normalized targets mean: -0.491\n",
      "Test point log value: -2.129276\n",
      "Back to linear: 7.425478e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lscratch/cjesus/tmp/ipykernel_3980512/514280426.py:21: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  print(f\"Value preservation ratio: {filtered_test_value/original_test_value:.6f} (should be ~1.0)\")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== B) DATASET AFTER LOADING AND FILTERING ===\")\n",
    "\n",
    "# Load dataset (same as training)\n",
    "dataset = PhotonSimDataset(data_path)\n",
    "\n",
    "print(f\"Filtered data shape: {dataset.data['inputs'].shape}\")\n",
    "print(f\"Raw targets min: {dataset.data['targets'].min():.6e}\")\n",
    "print(f\"Raw targets max: {dataset.data['targets'].max():.6e}\")\n",
    "print(f\"Raw targets mean: {dataset.data['targets'].mean():.6e}\")\n",
    "print(f\"Raw targets median: {np.median(dataset.data['targets']):.6e}\")\n",
    "\n",
    "# Find our test point in the filtered dataset\n",
    "test_coords_array = np.array(test_coords)\n",
    "distances = np.linalg.norm(dataset.data['inputs'] - test_coords_array, axis=1)\n",
    "closest_idx = np.argmin(distances)\n",
    "closest_coords = dataset.data['inputs'][closest_idx]\n",
    "filtered_test_value = dataset.data['targets'][closest_idx, 0]\n",
    "\n",
    "print(f\"\\nClosest point in dataset: E={closest_coords[0]:.0f} MeV, θ={np.degrees(closest_coords[1]):.1f}°, d={closest_coords[2]:.0f} mm\")\n",
    "print(f\"Filtered test value: {filtered_test_value:.6e}\")\n",
    "print(f\"Value preservation ratio: {filtered_test_value/original_test_value:.6f} (should be ~1.0)\")\n",
    "\n",
    "# Check log-normalized targets\n",
    "print(f\"\\nLog-normalized targets min: {dataset.data['targets_log'].min():.3f}\")\n",
    "print(f\"Log-normalized targets max: {dataset.data['targets_log'].max():.3f}\")\n",
    "print(f\"Log-normalized targets mean: {dataset.data['targets_log'].mean():.3f}\")\n",
    "\n",
    "log_test_value = dataset.data['targets_log'][closest_idx, 0]\n",
    "print(f\"Test point log value: {log_test_value:.6f}\")\n",
    "print(f\"Back to linear: {10**log_test_value:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearPhotonSimDataset Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LinearPhotonSimDataset WRAPPER ===\n",
      "LinearDataset batch inputs shape: (100, 3)\n",
      "LinearDataset batch targets shape: (100, 1)\n",
      "LinearDataset targets min: 3.056105e-03\n",
      "LinearDataset targets max: 2.081016e+02\n",
      "LinearDataset targets mean: 7.606782e+00\n",
      "\n",
      "Sample target from linear dataset: 1.450210e+00\n",
      "Ratio to original table scale: 0.266449\n",
      "\n",
      "Wrapper method tests:\n",
      "  get_sample_input() works: True\n",
      "  has_validation: True\n",
      "  data_type: h5_lookup\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== LinearPhotonSimDataset WRAPPER ===\")\n",
    "\n",
    "# Create the wrapper used in training\n",
    "class LinearPhotonSimDataset:\n",
    "    \"\"\"Wrapper to make dataset return linear values instead of log values\"\"\"\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        # Copy necessary attributes directly\n",
    "        self.data = base_dataset.data\n",
    "        self.train_indices = base_dataset.train_indices\n",
    "        self.val_indices = base_dataset.val_indices\n",
    "        self.normalized_bounds = base_dataset.normalized_bounds\n",
    "        self.metadata = base_dataset.metadata\n",
    "        self.energy_range = base_dataset.energy_range\n",
    "        self.angle_range = base_dataset.angle_range\n",
    "        self.distance_range = base_dataset.distance_range\n",
    "        self.data_type = base_dataset.data_type\n",
    "        self.data_path = base_dataset.data_path\n",
    "    \n",
    "    def get_batch(self, batch_size, rng, split='train', normalized=True):\n",
    "        # Get normalized inputs but LINEAR targets\n",
    "        if split == 'train':\n",
    "            indices = self.train_indices\n",
    "        else:\n",
    "            indices = self.val_indices\n",
    "            \n",
    "        # Random sampling\n",
    "        batch_indices = jax.random.choice(rng, indices, shape=(batch_size,))\n",
    "        \n",
    "        # Get normalized inputs\n",
    "        inputs = self.data['inputs_normalized'][batch_indices]\n",
    "        # Get LINEAR targets (not log!)\n",
    "        targets = self.data['targets'][batch_indices]\n",
    "        \n",
    "        return jnp.array(inputs), jnp.array(targets)\n",
    "    \n",
    "    def get_sample_input(self):\n",
    "        \"\"\"Get a sample input for model initialization.\"\"\"\n",
    "        return self.base_dataset.get_sample_input()\n",
    "    \n",
    "    def get_full_data(self, split='train', normalized=True):\n",
    "        \"\"\"Get full dataset for a given split.\"\"\"\n",
    "        return self.base_dataset.get_full_data(split=split, normalized=normalized)\n",
    "    \n",
    "    def denormalize_inputs(self, inputs):\n",
    "        \"\"\"Convert normalized inputs back to original scale.\"\"\"\n",
    "        return self.base_dataset.denormalize_inputs(inputs)\n",
    "    \n",
    "    def denormalize_targets(self, targets_log):\n",
    "        \"\"\"Convert log-normalized targets back to original scale.\"\"\"\n",
    "        return self.base_dataset.denormalize_targets(targets_log)\n",
    "    \n",
    "    @property\n",
    "    def has_validation(self):\n",
    "        \"\"\"Check if dataset has validation split.\"\"\"\n",
    "        return self.base_dataset.has_validation\n",
    "\n",
    "linear_dataset = LinearPhotonSimDataset(dataset)\n",
    "\n",
    "# Test a batch from LinearPhotonSimDataset\n",
    "rng = jax.random.PRNGKey(42)\n",
    "test_inputs, test_targets = linear_dataset.get_batch(100, rng, split='train')\n",
    "\n",
    "print(f\"LinearDataset batch inputs shape: {test_inputs.shape}\")\n",
    "print(f\"LinearDataset batch targets shape: {test_targets.shape}\")\n",
    "print(f\"LinearDataset targets min: {float(test_targets.min()):.6e}\")\n",
    "print(f\"LinearDataset targets max: {float(test_targets.max()):.6e}\")\n",
    "print(f\"LinearDataset targets mean: {float(test_targets.mean()):.6e}\")\n",
    "\n",
    "# Check if our test point is in this batch\n",
    "test_target_in_batch = float(test_targets[0, 0])  # First sample, first element\n",
    "print(f\"\\nSample target from linear dataset: {test_target_in_batch:.6e}\")\n",
    "print(f\"Ratio to original table scale: {test_target_in_batch/dataset.data['targets'].mean():.6f}\")\n",
    "\n",
    "# Test the wrapper methods\n",
    "print(f\"\\nWrapper method tests:\")\n",
    "print(f\"  get_sample_input() works: {linear_dataset.get_sample_input() is not None}\")\n",
    "print(f\"  has_validation: {linear_dataset.has_validation}\")\n",
    "print(f\"  data_type: {linear_dataset.data_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Load Trained Model and Check Training-time Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== C) TRAINED MODEL PREDICTIONS ===\n",
      "Output directory exists: True\n",
      "Checkpoint files found: 31\n",
      "  - checkpoint_step_2500.npz\n",
      "  - checkpoint_step_0.npz\n",
      "  - checkpoint_step_8500.npz\n",
      "  - checkpoint_step_9500.npz\n",
      "  - checkpoint_step_6000.npz\n",
      "✅ Trainer loaded successfully\n",
      "Trainer state is None: False\n",
      "\n",
      "SIREN predictions shape: (100, 1)\n",
      "SIREN predictions min: 6.966487e-05\n",
      "SIREN predictions max: 9.999993e-01\n",
      "SIREN predictions mean: 1.485371e-01\n",
      "\n",
      "Sample comparison:\n",
      "  Target: 1.450210e+00\n",
      "  SIREN:  9.450511e-02\n",
      "  Ratio (SIREN/Target): 0.065166\n",
      "\n",
      "Loss on this batch:\n",
      "  MSE: 9.511873e+02\n",
      "  Scaled (×1000): 9.511872e+05\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== C) TRAINED MODEL PREDICTIONS ===\")\n",
    "\n",
    "# Load the trained model (same as in the main notebook)\n",
    "config = TrainingConfig(\n",
    "    hidden_features=256,\n",
    "    hidden_layers=3,\n",
    "    w0=30.0,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.0,\n",
    "    batch_size=65536,\n",
    "    num_steps=5000,\n",
    "    use_patience_scheduler=True,\n",
    "    patience=20,\n",
    "    lr_reduction_factor=0.5,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "output_dir = Path('output') / 'photonsim_siren_training'\n",
    "\n",
    "try:\n",
    "    print(f\"Output directory exists: {output_dir.exists()}\")\n",
    "    if output_dir.exists():\n",
    "        checkpoint_files = list(output_dir.glob('*.npz'))\n",
    "        print(f\"Checkpoint files found: {len(checkpoint_files)}\")\n",
    "        for f in checkpoint_files[:5]:  # Show first 5\n",
    "            print(f\"  - {f.name}\")\n",
    "    \n",
    "    # Initialize trainer with linear dataset\n",
    "    trainer = SIRENTrainer(\n",
    "        linear_dataset,\n",
    "        config,\n",
    "        output_dir=output_dir,\n",
    "        resume_from_checkpoint=True\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Trainer loaded successfully\")\n",
    "    print(f\"Trainer state is None: {trainer.state is None}\")\n",
    "    \n",
    "    if trainer.state is not None:\n",
    "        # Test SIREN predictions on the same batch\n",
    "        siren_predictions = trainer.predict(test_inputs)\n",
    "        \n",
    "        print(f\"\\nSIREN predictions shape: {siren_predictions.shape}\")\n",
    "        print(f\"SIREN predictions min: {float(siren_predictions.min()):.6e}\")\n",
    "        print(f\"SIREN predictions max: {float(siren_predictions.max()):.6e}\")\n",
    "        print(f\"SIREN predictions mean: {float(siren_predictions.mean()):.6e}\")\n",
    "        \n",
    "        # Compare SIREN vs targets for the same batch\n",
    "        siren_sample = float(siren_predictions[0, 0])  # First prediction\n",
    "        target_sample = float(test_targets[0, 0])      # First target\n",
    "        \n",
    "        print(f\"\\nSample comparison:\")\n",
    "        print(f\"  Target: {target_sample:.6e}\")\n",
    "        print(f\"  SIREN:  {siren_sample:.6e}\")\n",
    "        print(f\"  Ratio (SIREN/Target): {siren_sample/target_sample:.6f}\")\n",
    "        \n",
    "        # Check training loss on this batch\n",
    "        mse_loss = jnp.mean((siren_predictions - test_targets) ** 2)\n",
    "        scaled_loss = mse_loss * 1000.0  # Same scaling as training\n",
    "        \n",
    "        print(f\"\\nLoss on this batch:\")\n",
    "        print(f\"  MSE: {float(mse_loss):.6e}\")\n",
    "        print(f\"  Scaled (×1000): {float(scaled_loss):.6e}\")\n",
    "    else:\n",
    "        print(\"❌ Trainer state is None - checkpoint loading failed\")\n",
    "        siren_predictions = None\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not load trainer: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    siren_predictions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Analyzer Plotting Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== D) ANALYZER PLOTTING VALUES ===\n",
      "Analyzer table range: 0.000000e+00 to 9.871154e+02\n",
      "Analyzer table mean: 1.354046e+00\n",
      "\n",
      "500 MeV slice from analyzer:\n",
      "  Range: 0.000000e+00 to 8.824773e+02\n",
      "  Mean: 1.190471e+00\n",
      "\n",
      "Analyzer SIREN predictions on 500 MeV grid:\n",
      "  Range: 3.716993e-05 to 9.499058e-01\n",
      "  Mean: 7.441159e-02\n",
      "\n",
      "Corresponding table values:\n",
      "  Range: 0.000000e+00 to 4.031448e+00\n",
      "  Mean: 1.169120e+00\n",
      "\n",
      "SCALE COMPARISON (SIREN/Table): 6.364752e-02\n",
      "Scaling factor needed: 15.71\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== D) ANALYZER PLOTTING VALUES ===\")\n",
    "\n",
    "if siren_predictions is not None:\n",
    "    # Simulate what the analyzer does\n",
    "    \n",
    "    # 1. Load original table again (as analyzer does)\n",
    "    with h5py.File(data_path, 'r') as f:\n",
    "        analyzer_table = f['data/photon_table_density'][:]\n",
    "        analyzer_energy_centers = f['coordinates/energy_centers'][:]\n",
    "        analyzer_angle_centers = f['coordinates/angle_centers'][:]\n",
    "        analyzer_distance_centers = f['coordinates/distance_centers'][:]\n",
    "    \n",
    "    print(f\"Analyzer table range: {analyzer_table.min():.6e} to {analyzer_table.max():.6e}\")\n",
    "    print(f\"Analyzer table mean: {analyzer_table.mean():.6e}\")\n",
    "    \n",
    "    # 2. Pick same energy slice as our test\n",
    "    energy_500_idx = np.argmin(np.abs(analyzer_energy_centers - 500))  # 500 MeV\n",
    "    table_slice = analyzer_table[energy_500_idx, :, :]\n",
    "    \n",
    "    print(f\"\\n500 MeV slice from analyzer:\")\n",
    "    print(f\"  Range: {table_slice.min():.6e} to {table_slice.max():.6e}\")\n",
    "    print(f\"  Mean: {table_slice.mean():.6e}\")\n",
    "    \n",
    "    # 3. Create grid for SIREN evaluation (as analyzer does)\n",
    "    angle_mesh, distance_mesh = np.meshgrid(\n",
    "        analyzer_angle_centers, \n",
    "        analyzer_distance_centers, \n",
    "        indexing='ij'\n",
    "    )\n",
    "    \n",
    "    # Create coordinate grid for a small subset\n",
    "    n_test = 100  # Just test a small grid\n",
    "    angle_flat = angle_mesh.flatten()[:n_test]\n",
    "    distance_flat = distance_mesh.flatten()[:n_test]\n",
    "    energy_flat = np.full_like(angle_flat, 500.0)  # 500 MeV\n",
    "    \n",
    "    eval_coords = np.stack([energy_flat, angle_flat, distance_flat], axis=-1)\n",
    "    \n",
    "    # 4. Normalize coordinates (as analyzer does)\n",
    "    input_min = dataset.normalized_bounds['input_min']\n",
    "    input_max = dataset.normalized_bounds['input_max']\n",
    "    eval_coords_norm = 2 * ((eval_coords - input_min) / (input_max - input_min)) - 1\n",
    "    \n",
    "    # 5. Get SIREN predictions on this grid\n",
    "    analyzer_siren_predictions = trainer.predict(eval_coords_norm)\n",
    "    \n",
    "    print(f\"\\nAnalyzer SIREN predictions on 500 MeV grid:\")\n",
    "    print(f\"  Range: {analyzer_siren_predictions.min():.6e} to {analyzer_siren_predictions.max():.6e}\")\n",
    "    print(f\"  Mean: {analyzer_siren_predictions.mean():.6e}\")\n",
    "    \n",
    "    # 6. Compare with corresponding table values\n",
    "    table_subset = table_slice.flatten()[:n_test]\n",
    "    \n",
    "    print(f\"\\nCorresponding table values:\")\n",
    "    print(f\"  Range: {table_subset.min():.6e} to {table_subset.max():.6e}\")\n",
    "    print(f\"  Mean: {table_subset.mean():.6e}\")\n",
    "    \n",
    "    # 7. Scale comparison\n",
    "    scale_ratio = analyzer_siren_predictions.mean() / table_subset.mean()\n",
    "    print(f\"\\nSCALE COMPARISON (SIREN/Table): {scale_ratio:.6e}\")\n",
    "    print(f\"Scaling factor needed: {1/scale_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CRITICAL: WHAT TRAINING ACTUALLY SEES ===\n",
      "Checking what happens when normalized=True is passed to get_batch()\n",
      "\n",
      "Base dataset with normalized=True:\n",
      "  Inputs range: -1.000 to 1.000\n",
      "  Targets range: -2.515 to 2.318\n",
      "  Targets mean: -0.535\n",
      "\n",
      "Base dataset with normalized=False:\n",
      "  Inputs range: 0.1 to 4490.0\n",
      "  Targets range: 3.056e-03 to 2.081e+02\n",
      "  Targets mean: 7.607e+00\n",
      "\n",
      "LinearPhotonSimDataset wrapper:\n",
      "  Inputs range: -1.000 to 1.000\n",
      "  Targets range: 3.056e-03 to 2.081e+02\n",
      "  Targets mean: 7.607e+00\n",
      "\n",
      "🔍 KEY INSIGHT:\n",
      "   ✅ Base dataset with normalized=True returns targets > 1.0\n",
      "   ❌ BUT LinearPhotonSimDataset returns targets > 1.0!\n",
      "   ❌ Training sees larger targets but SIREN still caps at 1.0\n",
      "   ❌ This suggests architectural limitation in SIREN model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CRITICAL: WHAT TRAINING ACTUALLY SEES ===\")\n",
    "print(\"Checking what happens when normalized=True is passed to get_batch()\")\n",
    "\n",
    "# Test what the LinearPhotonSimDataset returns vs base dataset\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "# 1. Base dataset with normalized=True\n",
    "base_inputs, base_targets = dataset.get_batch(100, rng, split='train', normalized=True)\n",
    "print(f\"\\nBase dataset with normalized=True:\")\n",
    "print(f\"  Inputs range: {base_inputs.min():.3f} to {base_inputs.max():.3f}\")\n",
    "print(f\"  Targets range: {base_targets.min():.3f} to {base_targets.max():.3f}\")\n",
    "print(f\"  Targets mean: {base_targets.mean():.3f}\")\n",
    "\n",
    "# 2. Base dataset with normalized=False  \n",
    "base_inputs_raw, base_targets_raw = dataset.get_batch(100, rng, split='train', normalized=False)\n",
    "print(f\"\\nBase dataset with normalized=False:\")\n",
    "print(f\"  Inputs range: {base_inputs_raw.min():.1f} to {base_inputs_raw.max():.1f}\")\n",
    "print(f\"  Targets range: {base_targets_raw.min():.3e} to {base_targets_raw.max():.3e}\")\n",
    "print(f\"  Targets mean: {base_targets_raw.mean():.3e}\")\n",
    "\n",
    "# 3. LinearPhotonSimDataset wrapper \n",
    "linear_inputs, linear_targets = linear_dataset.get_batch(100, rng, split='train', normalized=True)\n",
    "print(f\"\\nLinearPhotonSimDataset wrapper:\")\n",
    "print(f\"  Inputs range: {linear_inputs.min():.3f} to {linear_inputs.max():.3f}\")\n",
    "print(f\"  Targets range: {linear_targets.min():.3e} to {linear_targets.max():.3e}\")\n",
    "print(f\"  Targets mean: {linear_targets.mean():.3e}\")\n",
    "\n",
    "print(f\"\\n🔍 KEY INSIGHT:\")\n",
    "if base_targets.max() <= 1.0:\n",
    "    print(f\"   ❌ Base dataset with normalized=True returns targets ≤ 1.0!\")\n",
    "    print(f\"   ❌ This explains why SIREN predictions max out at 1.0\")\n",
    "    print(f\"   ❌ Training sees targets in range [0, 1] but analyzer expects [0, 1000]\")\n",
    "else:\n",
    "    print(f\"   ✅ Base dataset with normalized=True returns targets > 1.0\")\n",
    "\n",
    "if linear_targets.max() > 1.0:\n",
    "    print(f\"   ❌ BUT LinearPhotonSimDataset returns targets > 1.0!\")\n",
    "    print(f\"   ❌ Training sees larger targets but SIREN still caps at 1.0\")\n",
    "    print(f\"   ❌ This suggests architectural limitation in SIREN model\")\n",
    "else:\n",
    "    print(f\"   ✅ LinearPhotonSimDataset also caps targets ≤ 1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY: DATA PIPELINE SCALE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "A) Original lookup table scale:\n",
      "   Range: 0.00e+00 to 9.87e+02\n",
      "   Mean: 1.35e+00\n",
      "\n",
      "B) Dataset after filtering:\n",
      "   Range: 2.53e-03 to 9.87e+02\n",
      "   Mean: 5.44e+00\n",
      "   Preservation: 4.020\n",
      "\n",
      "Linear dataset training targets:\n",
      "   Range: 3.06e-03 to 2.08e+02\n",
      "   Mean: 7.61e+00\n",
      "\n",
      "C) SIREN predictions during training:\n",
      "   Range: 6.97e-05 to 1.00e+00\n",
      "   Mean: 1.49e-01\n",
      "   SIREN/Target ratio: 0.020\n",
      "\n",
      "D) Analyzer plotting values:\n",
      "   Table: 1.17e+00\n",
      "   SIREN: 7.44e-02\n",
      "   Scale mismatch: 6.36e-02\n",
      "\n",
      "============================================================\n",
      "DIAGNOSIS:\n",
      "❌ SIREN vs training targets mismatch: 0.020\n",
      "❌ MAJOR scale mismatch in analyzer plotting\n",
      "   Issue: Analyzer uses different scale than training\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: DATA PIPELINE SCALE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nA) Original lookup table scale:\")\n",
    "print(f\"   Range: {density_table.min():.2e} to {density_table.max():.2e}\")\n",
    "print(f\"   Mean: {density_table.mean():.2e}\")\n",
    "\n",
    "print(f\"\\nB) Dataset after filtering:\")\n",
    "print(f\"   Range: {dataset.data['targets'].min():.2e} to {dataset.data['targets'].max():.2e}\")\n",
    "print(f\"   Mean: {dataset.data['targets'].mean():.2e}\")\n",
    "print(f\"   Preservation: {dataset.data['targets'].mean()/density_table.mean():.3f}\")\n",
    "\n",
    "if 'test_targets' in locals():\n",
    "    print(f\"\\nLinear dataset training targets:\")\n",
    "    print(f\"   Range: {test_targets.min():.2e} to {test_targets.max():.2e}\")\n",
    "    print(f\"   Mean: {test_targets.mean():.2e}\")\n",
    "\n",
    "if 'siren_predictions' in locals() and siren_predictions is not None:\n",
    "    print(f\"\\nC) SIREN predictions during training:\")\n",
    "    print(f\"   Range: {siren_predictions.min():.2e} to {siren_predictions.max():.2e}\")\n",
    "    print(f\"   Mean: {siren_predictions.mean():.2e}\")\n",
    "    print(f\"   SIREN/Target ratio: {siren_predictions.mean()/test_targets.mean():.3f}\")\n",
    "\n",
    "if 'analyzer_siren_predictions' in locals():\n",
    "    print(f\"\\nD) Analyzer plotting values:\")\n",
    "    print(f\"   Table: {table_subset.mean():.2e}\")\n",
    "    print(f\"   SIREN: {analyzer_siren_predictions.mean():.2e}\")\n",
    "    print(f\"   Scale mismatch: {scale_ratio:.2e}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSIS:\")\n",
    "if 'siren_predictions' in locals() and siren_predictions is not None:\n",
    "    training_ratio = siren_predictions.mean()/test_targets.mean()\n",
    "    if abs(training_ratio - 1.0) < 0.1:\n",
    "        print(\"✅ SIREN matches training targets well\")\n",
    "    else:\n",
    "        print(f\"❌ SIREN vs training targets mismatch: {training_ratio:.3f}\")\n",
    "        \n",
    "    if 'analyzer_siren_predictions' in locals():\n",
    "        if abs(scale_ratio) < 0.1:\n",
    "            print(\"❌ MAJOR scale mismatch in analyzer plotting\")\n",
    "            print(\"   Issue: Analyzer uses different scale than training\")\n",
    "        else:\n",
    "            print(\"✅ Analyzer scale seems reasonable\")\n",
    "            \n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
